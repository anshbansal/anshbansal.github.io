{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lesson 01 - Overview of A B Testing\n",
    "\n",
    "## Course Overview\n",
    "- how to run an A/B Test for businesses who have an online presence\n",
    "- allows you to run possible changes with users and see what performs better\n",
    "- scientific way to find what works best rather than relying on intuition\n",
    "- about\n",
    "    - how to design a test\n",
    "    - choose metrics\n",
    "    - analyze the results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Take two set of users and show them different versions\n",
    "    * One set - control set - will be shown existing product\n",
    "    * Another set - experiment set - will be shown the new product\n",
    "* Isn't useful for testing out new features but rather is meant for whether the change in this feature improved stuff or not\n",
    "* If the change's results take time to come into effect then A/B Testing won't help. Say you are a house rental website and you want to know how to increase referrals by the user. But referrals don't happen that often. So that would be a really long test if done and thus not so useful.\n",
    "\n",
    "## Examples of A/B testing in industry:\n",
    "\n",
    "### Examples mentioned in the video:\n",
    "\n",
    "* Google [tested 41 different shades of blue](http://www.nytimes.com/2009/03/01/business/01marissa.html?pagewanted=3).\n",
    "* Amazon initially decided to launch their first personalized product recommendations based on an [A/B test showing a huge revenue increase by adding that feature](http://www.exp-platform.com/Documents/GuideControlledExperiments.pdf). (See the second paragraph in the introduction.)\n",
    "* LinkedIn [tested whether to use the top slot on a user's stream for top news articles or an encouragement to add more contacts](http://engineering.linkedin.com/mobile/mobile-ab-testing-linkedin-how-members-shape-our-apps). (See the first paragraph in \"A/B testing with view based JSON\" section.)\n",
    "* Amazon determined that [every 100ms increase in page load time decreased sales by 1%](http://www.exp-platform.com/Documents/IEEEComputer2007OnlineExperiments.pdf). (In \"Secondary metrics\" section on the last page)\n",
    "* Googleâ€™s [latency results](http://googleresearch.blogspot.com/2009/06/speed-matters.html) showed a similar impact for a 100ms delay.\n",
    "\n",
    "### Other Examples:\n",
    "\n",
    "* Kayak [tested whether notifying users that their payment was encrypted would make users more or less likely to complete the payment](http://apptimize.com/blog/2014/03/kayaks-most-interesting-ab-test/).\n",
    "* Khan Academy tests [changes like letting students know how many other students are working on the exercise with them, or making it easier for students to fast-forward past skills they already have](http://apptimize.com/blog/2014/07/how-khan-academy-uses-ab-testing-to-improve-student-learning/). (See the question \"What is the most interesting A/B test you've seen so far?\")\n",
    "\n",
    "### When can you use A/B Testing\n",
    "- Website selling cars is not a good idea as it will take too long to see whether there is a result\n",
    "- Should launch Pro service? Not a good option for A/B testing as you cannot assign people randomly to control and experiment as people need to opt-in. \n",
    "- update brand/logo can be an emotional thing for users and you can do A/B testing but in the long term\n",
    "- testing layout of page is definitely something that you can A/B Test\n",
    "\n",
    "### Other Techniques\n",
    "Sometimes A/B testing isn't helpful. What to do in those cases?\n",
    "- can collect data that is complementary to doing A/B test. logs can be analyzed and a hypothesis can be developed and based on that can do \n",
    "\n",
    "A/B testing has been around a while. e.g. in medicine clinical trials are effectively A/B tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Example\n",
    "\n",
    "Audacity makes online courses for finance courses\n",
    "![](funnel.png)\n",
    "\n",
    "- Our experiment would be to test the hypothesis \"changing the start now button from orange to pink will increase how many students explore audicty's courses.\n",
    "- Which metric to choose?\n",
    "    - total number of courses completed? No as it would take too long to complete\n",
    "    - number of clicks? No. Say if one of the 2 groups has more number of users visits then it would give us incorrect results\n",
    "    - (no. of clicks) / (no. of page views) - click through rate\n",
    "    - (unique visitors who click) / (unique visitors to page) - click through probability\n",
    "- updated hypothesis is \"chaning the start now button from orange to pink will increase the click through probability\". \n",
    "- Why probability instead of rate?\n",
    "    - rate should be used when want to measure usability of website. say we have a button then the rate will tell us how often do users actually find that button.\n",
    "    - probability should be used when want to measure the impact. but for finding how often do people go exploring you don't want to take into account page refresh, double clicks etc.\n",
    "    \n",
    "## Repeating experiment\n",
    "- When we repeat experiments then if our results are too different then we would be surprised\n",
    "- \"too different\" is related to standard error of our sample\n",
    "- standard error defines how variable estimates would be if we take different samples with the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Distribution\n",
    "- When should we be surprised by the changes in actual results vs. actual results?\n",
    "- Depends on the distribution\n",
    "\n",
    "## Binomial Distribution\n",
    "We flip a coin multiple times. We have a probability of how it will turn up. As we keep on increasing the flips it approaches a normal distribution\n",
    "\n",
    "![](binomial_distribution.png)\n",
    "\n",
    "Try using [this website](http://databits.io/bits/fbmx-binomial-distribution) to get a better feel for the binomial distribution. At the top, you can choose n (the number of events to generate) and p (the probability of success for each event) for two binomial distributions and compare them. The site will show one distribution as bars and the other as dots, and two graphs will be shown. The top graph shows the probability that exactly some number of successes k will occur for each k, and the bottom graph shows the cumulative probability that at k or fewer successes will occur (so the probability on the far right (k >= n) will always be 1).\n",
    "\n",
    "At the bottom, you can play a game where you throw a dice many times (with various definitions of success), keeping track of how many successes come up. This will let you see how the distribution you get resembles the binomial more and more as you keep playing.\n",
    "\n",
    "![](binomial_distribution2.png)\n",
    "\n",
    "## Calculating Confidence Intervals\n",
    "\n",
    "- We can use the knowledge that we will be using binomaial distribution to find confidence interval. \n",
    "- This makes it possible for us to find the interval around mean where we expect values to fall\n",
    "- Steps are\n",
    "    - Calculate Bias. This is center of confidence interval\n",
    "    - Need to calculate the width of confidence interval which is the SE of distribution\n",
    "- Example is below for calculating 99% confidence interval\n",
    "\n",
    "### Bias\n",
    "\n",
    "Estimate of probability\n",
    "\n",
    "$\\hat{p} = X / N$\n",
    "\n",
    "Instead of binomial distribution we can use normal distribution if \n",
    "\n",
    "$N * \\hat{p} > 5$\n",
    "\n",
    "$N * (1 - \\hat{p}) > 5$\n",
    "\n",
    "But for small probabilities the first one is more important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "N = 2000\n",
    "X = 300\n",
    "```\n",
    "\n",
    "$\\hat{p}$ = (300/2000) = 0.15\n",
    "\n",
    "$N * \\hat{p} = 2000 * 0.15 = 300$\n",
    "\n",
    "$N * (1 - \\hat{p}) = 2000 * 0.85 = 1700$\n",
    "\n",
    "As both are greater than 5 we can consider using a normal distribution instead of binomial distribution.\n",
    "\n",
    "\n",
    "$SE = \\sqrt{ (\\hat{p} * (1 - \\hat{p})) / N } = \\sqrt{ (0.15 * 0.85) / 2000 } = 0.007984359711335657$\n",
    "\n",
    "```\n",
    "margin of error (m) = SE * Z\n",
    "                    = 0.00798 * 2.575\n",
    "                    = 0.0205485\n",
    "```\n",
    "                    \n",
    "$Lower Bound  = \\hat{p} - m = 0.15 - 0.0205485 = 0.1294515$\n",
    "\n",
    "$Upper Bound = \\hat{p} + m = 0.15 + 0.0205485 = 0.1705485$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the SE is dependent on both\n",
    "- proportion of successes\n",
    "- number of samples\n",
    "\n",
    "So to decide how many samples we want to calculate we should consider proportion of successes. Will be covered again later in detail.\n",
    "\n",
    "### Note \n",
    "So far what we have done is\n",
    "- take a sample of clicks on the original website\n",
    "- computed confidence interval before making change on the sample\n",
    "- this helps us establish statistical (in)significance after making the change\n",
    "\n",
    "\n",
    "## Esatblish statistical significance using Hypothesis Testing\n",
    "- Need to calculate how likely results are due to chance\n",
    "\n",
    "![](hypothesis_testing.png)\n",
    "\n",
    "Say we run an experiment of changing the checkout flow of online shopping website\n",
    "\n",
    "### Null hypothesis\n",
    "Both groups have the same probability of completing a checkout\n",
    "\n",
    "### Alternate hypothesis\n",
    "Both groups have different probability of completing a checkout\n",
    "\n",
    "## Comparing 2 samples\n",
    "\n",
    "When comparing 2 samples we need a SE that gives us a good comparison of both. We will use pooled SE for this purpose.\n",
    "\n",
    "![](comparing_2_samples.png)\n",
    "\n",
    "## Practical, or Substantive, Significance ($d_{min}$)\n",
    "- From a business perspective what change matters to us?\n",
    "- A statistician would say what is substantive alongwith being statistically significant\n",
    "- only statistically significance is not enough as that just measures repeatability\n",
    "- need to measure something from a business perspective also\n",
    "- e.g.\n",
    "    - new drug launch has costs - training, launching drugs\n",
    "- different size for different businesses. e.g.\n",
    "    - 5% - 15% may be necessary for medicine and traditional science\n",
    "    - 1% may be enough for google\n",
    "\n",
    "\n",
    "## Size vs Power Trade Off\n",
    "- we want to decide how many people go into the experiment. This is called statistical power\n",
    "- the smaller the change that you want to detect or higher the confidence that you want to have the more the size needs to be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many page views\n",
    "```\n",
    "alpha = P (reject null | null true)\n",
    "beta = P (fail to reject null | null false)\n",
    "```\n",
    "- alpha - falsely concluding there was a difference\n",
    "- beta - failing to detect a true difference\n",
    "\n",
    "As size increase the distribution gets tighter \n",
    "\n",
    "![](normal.png)\n",
    "\n",
    "thus `beta` gets lower (as there is less overlap) while `alpha` remains the same.\n",
    "\n",
    "```\n",
    "sensitivity = 1 - beta\n",
    "```\n",
    "\n",
    "### Calculating number of page views\n",
    "- built-in library\n",
    "- lookup answer in a table\n",
    "- online calculator\n",
    "\n",
    "\n",
    "Statistics textbooks frequently define power to mean the same thing as sensitivity, that is, 1 - beta. However, conversationally power often means the probability that your test draws the correct conclusions, and this probability depends on both alpha and beta. In this course, we'll use the second definition, and we'll use sensitivity to refer to 1 - beta.\n",
    "\n",
    "Use [this calculator](http://www.evanmiller.org/ab-testing/sample-size.html) to determine how many page views we'll need to collect in our experiment. Make sure to choose an absolute difference, not a relative difference.\n",
    "\n",
    "### How Does Number of Page Views Vary\n",
    "\n",
    "For first case we need same sensitivity\n",
    "![](number_of_page_views.png)\n",
    "\n",
    "After getting results we need to analyze them\n",
    "![](analyze_results.png)\n",
    "\n",
    "For various confidence intervals we decide whether we want to launch or whether we don't want to launch the change\n",
    "![](confidence_intervals.png)\n",
    "\n",
    "In case you are not certain then the risk needs to be communicated to the business decision makers. They need to consider other factors and consider whether they want to launch the test or not."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
