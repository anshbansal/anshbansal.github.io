{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lesson 01 - Naive Bayes\n",
    "\n",
    "## ML in The Google Self-Driving Car\n",
    "We will start with supervised classification and self-driving cars are result of supervised classification. Similar to how humans learn computers learn the same way when doing machine learning. You give them a lot of examples and they start figuring out how it works.\n",
    "\n",
    "## Quiz - Acerous/Non-Acerous\n",
    "When looking at new examples you need to figure out which parts we should be paying attention to. We call these parts as features.\n",
    "\n",
    "In machine learning we give you many examples. These examples have many features/attributes which can be used to describe them. Need to pick the right features and if those features give you the right answers then you can use those features to classify new examples.\n",
    "\n",
    "## Features and Labels Musical Example\n",
    "In machine learning we take features and try to produce labels.\n",
    "\n",
    "From music we don't take raw music but we take attributes that we call features. e.g. intensity, tempo, genre etc. \n",
    "\n",
    "Using these features we make 2 labels - like, don't like\n",
    "\n",
    "![](feature_visual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanley Terrain Classification\n",
    "We are considering a simple robot. We consider 2 features \n",
    "- is it flat or a hill?\n",
    "- ruggeddness of terrain - how much vehicle bumps up and down\n",
    "\n",
    "![](car_feature.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplots to Predictions\n",
    "\n",
    "Given a set of examples machine learning algorithms make a decision surface. This decision surface allows us to classify new points given previous examples.\n",
    "\n",
    "![](decision_surface.png)\n",
    "\n",
    "## Naive Bayes Decision Boundary in Python\n",
    "\n",
    "What we want to do is to take all the training data and make a decision boundary like below.\n",
    "\n",
    "![](naive_fast_forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started With sklearn\n",
    "\n",
    "We will use Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "\n",
    "print(clf.predict([[-0.8, -1], [4, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit and train are interchangeably used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "prediction = clf.predict([[-0.8, -1]])\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print accuracy_score(prediction, [0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Data\n",
    "In machine learning you need to train and test your algorithm on different data. If you use same data then it is possible to get 100% accuracy but that would fail with new data\n",
    "\n",
    "## Bayes Rule\n",
    "\n",
    "Let' use cancer example. Say if we have a specific cancer C \n",
    "\n",
    "P(C) = 0.01\n",
    "\n",
    "Test: \n",
    "- (Sensitivity): 90% positive if you have this cancer\n",
    "- (Specitivity): 90% negative if you don't have cancer\n",
    "\n",
    "Question:\n",
    "- Without any further symptons you take the test and test is positive then what is probability of having that cancer?\n",
    "\n",
    "![](cancer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08256880733944955"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_C = 0.01\n",
    "positive_when_cancer = (P_C * 0.9)\n",
    "positive_when_no_cancer = 0.1\n",
    "\n",
    "cancer_probability_given_test_positive = positive_when_cancer / (positive_when_cancer + positive_when_no_cancer)\n",
    "cancer_probability_given_test_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence we have some prior probability and then we get some evidence from the test. Based on this new information we get a posterior probability\n",
    "\n",
    "So it incorporates evidence from test into prior probability to arrive at posterior probability\n",
    "\n",
    "![](prior_posterior.png)\n",
    "\n",
    "![](bayes_rule_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Rule for Classification\n",
    "\n",
    "Bayes rule is used for \"Text Learning\" using \"Naive Bayes\". People use different words with different frequency. Given frequency by which people use the words and a block of text we can find the probability that the text was written by whom\n",
    "\n",
    "![](text_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[## Why is Naive Bayes Naive\n",
    "\n",
    "Because it does not consider word order but just the frequency of words\n",
    "\n",
    "## Naive Bayes Pros and Cons\n",
    "\n",
    "Pros\n",
    "\n",
    "- It's actually really easy to implement.\n",
    "- deals really great with great, big feature spaces there is between 20,000 and 200,000 words in the English language\n",
    "- And it, it's really simple to run, it's really efficient\n",
    "\n",
    "Cons\n",
    "\n",
    "- Yeah, so it can break. It breaks in funny ways. So phrases that encompass multiple words and have distinctive meanings don't work really well in Na√Øve Bayes.\n",
    "- So, historically when Google first sent it out, when people searched for Chicago Bulls, which is a sports team comprised of two words, Chicago Bulls Would show many images of bulls, animals, and of cities, like Chicago. But Chicago Bulls is something succinctly different.\n",
    "\n",
    "[J.K. Rowling related article](http://languagelog.ldc.upenn.edu/nll/?p=5315)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
