{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lesson 10 - Text Learning\n",
    "\n",
    "When learning from text the biggest problem is that different text have different length. A smaller email would require lesser features while longer email would require more features.\n",
    "\n",
    "## Bag of words\n",
    "- Make a dictonary of counts of all the words that we care about.\n",
    "- word order does not matter\n",
    "- long phrases give different vectors\n",
    "- complex phrases cannot be handled like \"chicago bulls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'be': 2, u'we': 15, u'company': 4, u'of': 11, u'it': 8, u'paid': 12, u'regards': 13, u'know': 9, u'in': 7, u'why': 16, u'advance': 0, u'don': 5, u'aseem': 1, u'car': 3, u'will': 17, u'hi': 6, u'late': 10, u'the': 14}\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 17)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 17)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 4)\t2\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 17)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "string1 = \"hi aseem the car will be late regards company\"\n",
    "string2 = \"hi company why will it be late I paid in advance regards aseem\"\n",
    "string3 = \"hi aseem we don't know why will it be late regards company of company\"\n",
    "\n",
    "email_list = [string1, string2, string3]\n",
    "\n",
    "vectorizer.fit(email_list)\n",
    "bag_of_words = vectorizer.transform(email_list)\n",
    "\n",
    "print vectorizer.vocabulary_\n",
    "print bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not all words are equal\n",
    "like the, hi etc.\n",
    "\n",
    "## stopwords\n",
    "- occur very frequently, low information and should be removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "len(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not all unique words different\n",
    "- unresponsive\n",
    "- response\n",
    "- responsivity\n",
    "- responsiveness\n",
    "- respond\n",
    "\n",
    "All of them can be passed through to get a root/stem - respon\n",
    "\n",
    "We don't need all of them as their meaning is only slightly different and we don't get information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "respons\n",
      "respons\n",
      "unrespons\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "print stemmer.stem(\"responsiveness\")\n",
    "print stemmer.stem(\"responsivity\")\n",
    "print stemmer.stem(\"unresponsive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order of operations in text processing\n",
    "Should do stemming before adding them to bag of words\n",
    "\n",
    "## Weighting by term frequency\n",
    "- TfIdf representation\n",
    "    - Tf - term frequency\n",
    "    - Idf - inverse document frequency\n",
    "    \n",
    "![](TfIdf.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
