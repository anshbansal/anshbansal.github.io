{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The starter code can be found in the final_project directory of the codebase that you downloaded for use with the mini-projects. Some relevant files: \n",
    "\n",
    "**poi_id.py** : Starter code for the POI identifier, you will write your analysis here. You will also submit a version of this file for your evaluator to verify your algorithm and results. \n",
    "\n",
    "**final_project_dataset.pkl** : The dataset for the project, more details below. \n",
    "\n",
    "**tester.py** : When you turn in your analysis for evaluation by Udacity, you will submit the algorithm, dataset and list of features that you use (these are created automatically in **poi_id.py**). The evaluator will then use this code to test your result, to make sure we see performance that’s similar to what you report. You don’t need to do anything with this code, but we provide it for transparency and for your reference. \n",
    "\n",
    "**emails_by_address** : this directory contains many text files, each of which contains all the messages to or from a particular email address. It is for your reference, if you want to create more advanced features based on the details of the emails dataset. You do not need to process the e-mail corpus in order to complete the project.\n",
    "\n",
    "# Steps to Success\n",
    "We will provide you with starter code that reads in the data, takes your features of choice, then puts them into a numpy array, which is the input form that most sklearn functions assume. Your job is to engineer the features, pick and tune an algorithm, and to test and evaluate your identifier. Several of the mini-projects were designed with this final project in mind, so be on the lookout for ways to use the work you’ve already done.\n",
    "\n",
    "As preprocessing to this project, we've combined the Enron email and financial data into a dictionary, where each key-value pair in the dictionary corresponds to one person. The dictionary key is the person's name, and the value is another dictionary, which contains the names of all the features and their values for that person. The features in the data fall into three major types, namely financial features, email features and POI labels.\n",
    "\n",
    "**financial features**: ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees'] (all units are in US dollars)\n",
    "\n",
    "**email features**: ['to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi'] (units are generally number of emails messages; notable exception is ‘email_address’, which is a text string)\n",
    "\n",
    "**POI label**: [‘poi’] (boolean, represented as integer)\n",
    "\n",
    "You are encouraged to make, transform or rescale new features from the starter features. If you do this, you should store the new feature to my_dataset, and if you use the new feature in the final algorithm, you should also add the feature name to my_feature_list, so your evaluator can access it during testing. For a concrete example of a new feature that you could add to the dataset, refer to the lesson on Feature Selection.\n",
    "\n",
    "In addition, we advise that you keep notes as you work through the project. As part of your project submission, you will compose answers to a series of questions (given on the next page) to understand your approach towards different aspects of the analysis. Your thought process is, in many ways, more important than your final project and we will by trying to probe your thought process in these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free form questions \n",
    "https://docs.google.com/document/d/1NDgi1PrNJP7WTbfSUuRUnz8yzs5nGVTSzpO7oeNTEWA/pub?embedded=true\n",
    "\n",
    "Rubric\n",
    "https://review.udacity.com/#!/rubrics/27/view\n",
    "\n",
    "A list of Web sites, books, forums, blog posts, github repositories etc. that you referred to or used in this submission (add N/A if you did not use such resources). Please carefully read the following statement and include it in your document “I hereby confirm that this submission is my work. I have cited above the origins of any parts of the submission that were taken from Websites, books, forums, blog posts, github repositories, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings\n",
    "- deferred_income,director_fees,restricted_stock_deferred give precison and recall > 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "import time\n",
    "def print_time(name, start_time):\n",
    "    print \"{} time: {}s\".format(name, round(time.time() - start_time, 3))\n",
    "\n",
    "DISPLAY_STRING = \"{:>0.{display_precision}f}\\t\\t\"\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "Accuracy\\tPrecision\\tRecall\\t\\tF1\\t\\tF2\\n\\\n",
    "{0}{0}{0}{0}{0}\".format(\n",
    "    DISPLAY_STRING\n",
    ")\n",
    "\n",
    "RESULTS_FORMAT_STRING = \"\\\n",
    "Total predictions, \\tTrue positives, \\tFalse positives, \\tFalse negatives, \\tTrue negatives \\n\\\n",
    "{:4d}\\t\\t\\t{:4d}\\t\\t\\t{:4d}\\t\\t\\t{:4d}\\t\\t\\t{:4d}\"\n",
    "\n",
    "def ratio(numerator, denominator):\n",
    "    if numerator == 0 or denominator == 0:\n",
    "        return 0\n",
    "    \n",
    "    return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def get_quadrant(true_negatives, false_negatives, true_positives, false_positives, predictions, labels_test):\n",
    "    for prediction, truth in zip(predictions, labels_test):\n",
    "        if prediction == 0 and truth == 0:\n",
    "            true_negatives += 1\n",
    "        elif prediction == 0 and truth == 1:\n",
    "            false_negatives += 1\n",
    "        elif prediction == 1 and truth == 0:\n",
    "            false_positives += 1\n",
    "        elif prediction == 1 and truth == 1:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "            print \"All predictions should take value 0 or 1.\"\n",
    "            print \"Evaluating performance for processed predictions:\"\n",
    "            break\n",
    "    \n",
    "    return true_negatives, false_negatives, true_positives, false_positives\n",
    "\n",
    "\n",
    "def get_stats(true_negatives, false_negatives, true_positives, false_positives):\n",
    "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "    accuracy = ratio(true_positives + true_negatives, total_predictions)\n",
    "    precision = ratio(true_positives, true_positives + false_positives)\n",
    "    recall = ratio(true_positives, true_positives + false_negatives)\n",
    "    f1 = ratio(2.0 * true_positives, 2 * true_positives + false_positives + false_negatives)\n",
    "    f2 = ratio(5 * precision * recall, (4 * precision) + recall)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'f2': f2,\n",
    "        'total_predictions': total_predictions,\n",
    "        'true_positives': true_positives,\n",
    "        'false_positives': false_positives,\n",
    "        'false_negatives': false_negatives,\n",
    "        'true_negatives': true_negatives\n",
    "    }\n",
    "\n",
    "def get_labels_and_features(dataset, feature_list):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    return targetFeatureSplit(data)\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "        \n",
    "    labels, features = get_labels_and_features(dataset, feature_list)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    \n",
    "\n",
    "    for train_idx, test_idx in cv:\n",
    "        #print \"counting {}, {}, {}, {}\".format(true_negatives, false_negatives, true_positives, false_positives)\n",
    "        \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        clf.fit(features_train, labels_train)\n",
    "        \n",
    "        predictions = clf.predict(features_test)\n",
    "        \n",
    "        true_negatives, false_negatives, true_positives, false_positives = get_quadrant(\n",
    "            true_negatives, false_negatives, true_positives, false_positives, predictions, labels_test\n",
    "        )\n",
    "        \n",
    "    return get_stats(true_negatives, false_negatives, true_positives, false_positives)\n",
    "\n",
    "def test_classifier_fast(clf, dataset, feature_list):\n",
    "    labels, features = get_labels_and_features(dataset, feature_list)\n",
    "\n",
    "    t0 = time.time()\n",
    "    clf.fit(features_train, labels_train)\n",
    "    print_time(\"training\", t0)\n",
    "\n",
    "    t0 = time.time()\n",
    "    predictions = clf.predict(features_test)\n",
    "    print_time(\"predictions\", t0)\n",
    "\n",
    "    true_negatives, false_negatives, true_positives, false_positives = get_quadrant(\n",
    "        0, 0, 0, 0, predictions, labels_test\n",
    "    )\n",
    "\n",
    "    return get_stats(true_negatives, false_negatives, true_positives, false_positives)\n",
    "\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    with open(CLF_PICKLE_FILENAME, \"r\") as clf_infile:\n",
    "        clf = pickle.load(clf_infile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"r\") as dataset_infile:\n",
    "        dataset = pickle.load(dataset_infile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"r\") as featurelist_infile:\n",
    "        feature_list = pickle.load(featurelist_infile)\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "    \n",
    "#test_classifier(clf, my_dataset, features_list)\n",
    "#print(PERF_FORMAT_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import time\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "STORE_LOCATION = \"intermediate_results/\"\n",
    "\n",
    "def store_dictionary(dictionary, file_name):\n",
    "    with open(STORE_LOCATION + file_name, 'w') as f:\n",
    "        pickle.dump(dictionary, f)\n",
    "    \n",
    "def dict_to_df(dictionary):\n",
    "    return pd.DataFrame.from_dict(dictionary, orient = 'index')\n",
    "\n",
    "def read_dictionary(dict_file_name):\n",
    "    with open(STORE_LOCATION + dict_file_name, 'r') as f:\n",
    "        tmp_dict = pickle.load(f)\n",
    "    return tmp_dict, dict_to_df(tmp_dict)\n",
    "\n",
    "def result_for_features(dictionary, features):\n",
    "    return dictionary[feature_list_to_key(features)]\n",
    "\n",
    "def get_rows_above_threshold(df, \n",
    "                             threshold_precision = 0.3,\n",
    "                             threshold_recall = 0.3, \n",
    "                             threshold_accuracy = 0.8, \n",
    "                             metrics = ['precision', 'recall', 'accuracy']):\n",
    "    condition = ((df.precision >= threshold_precision) & \n",
    "                 (df.recall >= threshold_recall) & \n",
    "                 (df.accuracy > threshold_accuracy))\n",
    "    return df[condition].loc[:, metrics]\n",
    "#     return df[condition].loc[:, :]\n",
    "\n",
    "\n",
    "def get_combination_features(n):\n",
    "    for combination_length in xrange(1, n + 1):\n",
    "        for comb in itertools.combinations(all_features, combination_length):\n",
    "            yield ['poi'] + list(comb)\n",
    "            \n",
    "def get_number_combinations(n):\n",
    "    return len(list(get_combination_features(n))) - 1\n",
    "\n",
    "def feature_list_to_key(_features_list):\n",
    "    copy = _features_list[:]\n",
    "    copy.remove('poi')\n",
    "    return \",\".join(sorted(copy))\n",
    "\n",
    "def estimate_time(current, total, time_taken):\n",
    "    if current == 0:\n",
    "        return 0\n",
    "    return (time_taken * total) / float(current)\n",
    "\n",
    "def report_time(current, total, begin_time, estimates, how_often = 1):\n",
    "    estimates[:-10] = []\n",
    "    time_taken = time.time() - begin_time\n",
    "    \n",
    "    start_estimate = estimate_time(current, total, time_taken)\n",
    "    if current == 0:\n",
    "        estimate = start_estimate\n",
    "    else:\n",
    "        estimate = np.mean([start_estimate, np.mean(estimates)])\n",
    "    estimates.append(estimate)\n",
    "    \n",
    "    if (current > 0) and (current % how_often == 0):\n",
    "        print(\"reached {:5d} / {:5d} || done {:6.2f} %  || Time taken {:10.2f} / {:10.2f}\".format(\n",
    "                current, total, ratio(time_taken * 100, estimate), time_taken, estimate\n",
    "            ))\n",
    "        \n",
    "def at_most_features(df, n):\n",
    "    return df[df.index.map(lambda x: len(x.split(',')) <= n)]\n",
    "\n",
    "def above_threshold_at_most(df, p, r, a, n):\n",
    "    return at_most_features(get_rows_above_threshold(df, p, r, a, \n",
    "                                                     ['precision', 'recall', 'accuracy', 'f1', 'f2']), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me start by copying the code from the main file and breaking it up here so that I can arrange it and play around with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "projects_home = '/home/aseem/projects/ud120-projects'\n",
    "final_project_home = projects_home + '/final_project/'\n",
    "sys.path.append(final_project_home)\n",
    "sys.path.append(projects_home + '/tools/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.88,\n",
       " 'f1': 0.6232339089481946,\n",
       " 'f2': 0.6062919975565059,\n",
       " 'false_negatives': 809,\n",
       " 'false_positives': 631,\n",
       " 'precision': 0.6536772777167947,\n",
       " 'recall': 0.5955,\n",
       " 'total_predictions': 12000,\n",
       " 'true_negatives': 9369,\n",
       " 'true_positives': 1191}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data#, test_classifier\n",
    "\n",
    "with open(final_project_home + \"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    \n",
    "    #remove outliers\n",
    "    del data_dict['TOTAL']\n",
    "    \n",
    "    #Replace NaN with 0\n",
    "    for key, value in data_dict.iteritems():\n",
    "        for k, v in value.iteritems():\n",
    "            if v == 'NaN':\n",
    "                value[k] = 0\n",
    "\n",
    "    #remove features with not enough value\n",
    "    columns_to_remove = ['email_address', 'loan_advances', 'restricted_stock_deferred', 'director_fees']\n",
    "    for key, value in data_dict.iteritems():\n",
    "        for column in columns_to_remove:\n",
    "            del value[column]\n",
    "    \n",
    "    #Create new features\n",
    "    for key, value in data_dict.iteritems():\n",
    "        value['fraction_poi_to_this_person'] = ratio(value['from_poi_to_this_person'], value['to_messages'])\n",
    "        value['fraction_from_this_person_to_poi'] = ratio(value['from_this_person_to_poi'], value['from_messages'])\n",
    "        value['total_income'] = value['salary'] + value['bonus'] + value['long_term_incentive'] + \\\n",
    "                value['other'] + value['expenses']\n",
    "        \n",
    "#Store to my Dataset for easy export\n",
    "my_dataset = data_dict\n",
    "\n",
    "#Make final classifier for export\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "steps = [\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors = 1))\n",
    "]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf = Pipeline(steps)\n",
    "\n",
    "\n",
    "features_list = [\n",
    "    'poi',\n",
    "    'exercised_stock_options',\n",
    "    'fraction_from_this_person_to_poi',\n",
    "    'from_messages',\n",
    "    'from_poi_to_this_person'\n",
    "]\n",
    "\n",
    "#dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "#clf, my_dataset, feature_list = load_classifier_and_data()\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exercised_stock_options,fraction_from_this_person_to_poi,from_messages,from_poi_to_this_person'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "all_features = [\n",
    "#     'poi',\n",
    "     'salary',\n",
    "     'to_messages',\n",
    "     'deferral_payments',\n",
    "     'total_payments',\n",
    "     'exercised_stock_options',\n",
    "     'bonus',\n",
    "     'restricted_stock',\n",
    "     'shared_receipt_with_poi',\n",
    "#      'restricted_stock_deferred',\n",
    "     'total_stock_value',\n",
    "     'expenses',\n",
    "#      'loan_advances',\n",
    "     'from_messages',\n",
    "     'other',\n",
    "     'from_this_person_to_poi',\n",
    "#      'director_fees',\n",
    "     'deferred_income',\n",
    "     'long_term_incentive',\n",
    "    'from_poi_to_this_person',\n",
    "    'fraction_poi_to_this_person',\n",
    "    'fraction_from_this_person_to_poi',\n",
    "    'total_income'\n",
    "]\n",
    "\n",
    "features_list = [\n",
    "    'poi',\n",
    "    'exercised_stock_options',\n",
    "    'fraction_from_this_person_to_poi',\n",
    "    'from_messages',\n",
    "    'from_poi_to_this_person'\n",
    "]\n",
    "\n",
    "feature_list_to_key(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "    \n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "print len(data)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#clf = GaussianNB()\n",
    "\n",
    "from sklearn import svm\n",
    "#clf = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#clf = KNeighborsClassifier(n_neighbors = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_neighbors1(number_features = 1):\n",
    "    \n",
    "    k_neigh_result_simple_test = {}\n",
    "    estimates = []\n",
    "    errors = []\n",
    "    \n",
    "    total_iterations = get_number_combinations(number_features)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for i, tmp_list in enumerate(get_combination_features(number_features)):\n",
    "        n_neighbors = 1\n",
    "    #     for n_neighbors in xrange(1, 3):\n",
    "\n",
    "        clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "        #print \"features_list is {}\".format(features_list)\n",
    "\n",
    "        start = time.time()\n",
    "        #result = test_classifier_fast(clf, my_dataset, features_list)\n",
    "        result = test_classifier(clf, my_dataset, tmp_list)\n",
    "        #result['n_neighbors'] = n_neighbors\n",
    "        result['time_taken'] = time.time() - start\n",
    "        \n",
    "        \n",
    "        key = \"{}\".format(feature_list_to_key(tmp_list))\n",
    "    #     key = \"{},{}\".format(feature_list_to_key(tmp_list), n_neighbors)\n",
    "        k_neigh_result_simple_test[key] = result\n",
    "        \n",
    "        if i > 0 and ((i <= 100 and i % 5 == 0) or i % 25 == 0):\n",
    "            report_time(i, total_iterations, t0, estimates)\n",
    "        \n",
    "    else:\n",
    "        print(\"*\" * 30)\n",
    "        print(\"total   {:4d} at {:10.4f}\".format(i, time.time() - t0))\n",
    "    \n",
    "    return k_neigh_result_simple_test\n",
    "    \n",
    "#k_neigh_result_simple_test = do_neighbors1(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_neigh_results, k_neigh_df = read_dictionary('features_5_raw_features_k_neigh.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** naive bayes\n",
      "\n",
      "\n",
      "**** k neigh neighbors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def print_required1(df):\n",
    "    print above_threshold_at_most(df, 0.3, 0.3, 0.8, 3).describe()\n",
    "\n",
    "print '**** naive bayes'\n",
    "print \"\"\n",
    "#print_required1(naive_bayes_df)\n",
    "print \"\"\n",
    "print \"**** k neigh neighbors\"\n",
    "print \"\"\n",
    "#print_required1(k_neigh_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use features found using Naive Bayes brute force\n",
    "list_of_features_list = []\n",
    "for index in get_rows_above_threshold(naive_bayes_df, 0.4).index:\n",
    "    list_of_features_list.append(['poi'] + index.split(','))\n",
    "    \n",
    "len(list_of_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score\n",
    "\n",
    "def do_grid_search1():\n",
    "    feature_list = ['poi', 'salary']\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [1e-1, 1e0, 1e1]\n",
    "    }\n",
    "    clf = GridSearchCV(\n",
    "        svm.SVC(kernel='linear'), \n",
    "        scoring = make_scorer(precision_score),\n",
    "        param_grid=param_grid, \n",
    "        n_jobs=3, \n",
    "        verbose = 4\n",
    "    )\n",
    "    print_time(\"creation\", t0)\n",
    "\n",
    "    t0 = time.time()\n",
    "    labels, features = get_labels_and_features(my_dataset, feature_list)\n",
    "    features_train, features_test, labels_train, labels_test = \\\n",
    "        train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    print_time(\"split\", t0)\n",
    "\n",
    "    t0 = time.time()\n",
    "    clf.fit(features_train, labels_train)\n",
    "    print_time(\"training\", t0)\n",
    "    print \"best estimator {}\".format(clf.best_estimator_)\n",
    "\n",
    "    t0 = time.time()\n",
    "    predictions = clf.predict(features_test)\n",
    "    print_time(\"predictions\", t0)\n",
    "\n",
    "    return get_stats(*get_quadrant(\n",
    "        0, 0, 0, 0, predictions, labels_test\n",
    "    ))\n",
    "    \n",
    "#do_grid_search1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_svc_simple_test():\n",
    "    svc_result_simple_test = {}\n",
    "\n",
    "    for features_list in list_of_features_list:\n",
    "\n",
    "        print \"features_list is {}\".format(features_list)\n",
    "\n",
    "        start = time.time()\n",
    "        result = test_classifier_fast(clf, my_dataset, features_list)\n",
    "        result['time_taken'] = time.time() - start\n",
    "\n",
    "        print \"time_taken was {}\".format(result['time_taken'])\n",
    "        svc_result_simple_test[feature_list_to_key(features_list)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deferred_income,exercised_stock_options,from_messages</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.5</td>\n",
       "      <td>28.559094</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferred_income,exercised_stock_options,from_messages,from_poi_to_this_person</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.5</td>\n",
       "      <td>33.051872</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferred_income,exercised_stock_options,from_messages,from_this_person_to_poi</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.5</td>\n",
       "      <td>31.269867</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferred_income,exercised_stock_options,long_term_incentive,salary</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.5</td>\n",
       "      <td>33.846827</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferred_income,exercised_stock_options,long_term_incentive,total_stock_value</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.5</td>\n",
       "      <td>37.181807</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferred_income,expenses,from_messages,total_stock_value</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.5</td>\n",
       "      <td>41.274096</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferred_income,from_messages,long_term_incentive,total_stock_value</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.5</td>\n",
       "      <td>39.736730</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          f1      f2  \\\n",
       "deferred_income,exercised_stock_options,from_me...  0.363636  0.3125   \n",
       "deferred_income,exercised_stock_options,from_me...  0.363636  0.3125   \n",
       "deferred_income,exercised_stock_options,from_me...  0.363636  0.3125   \n",
       "deferred_income,exercised_stock_options,long_te...  0.363636  0.3125   \n",
       "deferred_income,exercised_stock_options,long_te...  0.363636  0.3125   \n",
       "deferred_income,expenses,from_messages,total_st...  0.363636  0.3125   \n",
       "deferred_income,from_messages,long_term_incenti...  0.363636  0.3125   \n",
       "\n",
       "                                                      recall  precision  \\\n",
       "deferred_income,exercised_stock_options,from_me...  0.285714        0.5   \n",
       "deferred_income,exercised_stock_options,from_me...  0.285714        0.5   \n",
       "deferred_income,exercised_stock_options,from_me...  0.285714        0.5   \n",
       "deferred_income,exercised_stock_options,long_te...  0.285714        0.5   \n",
       "deferred_income,exercised_stock_options,long_te...  0.285714        0.5   \n",
       "deferred_income,expenses,from_messages,total_st...  0.285714        0.5   \n",
       "deferred_income,from_messages,long_term_incenti...  0.285714        0.5   \n",
       "\n",
       "                                                    time_taken  accuracy  \n",
       "deferred_income,exercised_stock_options,from_me...   28.559094  0.758621  \n",
       "deferred_income,exercised_stock_options,from_me...   33.051872  0.758621  \n",
       "deferred_income,exercised_stock_options,from_me...   31.269867  0.758621  \n",
       "deferred_income,exercised_stock_options,long_te...   33.846827  0.758621  \n",
       "deferred_income,exercised_stock_options,long_te...   37.181807  0.758621  \n",
       "deferred_income,expenses,from_messages,total_st...   41.274096  0.758621  \n",
       "deferred_income,from_messages,long_term_incenti...   39.736730  0.758621  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_to_df(svc_result_simple_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store_dictionary(svc_result_simple_test, \"svc_on_naive_bayes_top_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features list is ['poi', 'deferred_income', 'exercised_stock_options', 'from_messages']\n",
      "clf = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), feature_list = ['poi', 'deferred_income', 'exercised_stock_options', 'from_messages'], folds = 10, verbose_at = 1\n"
     ]
    }
   ],
   "source": [
    "svc_results = {}\n",
    "for features_list in list_of_features_list:\n",
    "    folds = 10\n",
    "    start = time.time()\n",
    "    \n",
    "    clf = SVC(kernel='linear')\n",
    "    \n",
    "    result = test_classifier(clf, my_dataset, features_list, folds=folds, verbose=True, verbose_at=1)\n",
    "\n",
    "    result['folds'] = folds\n",
    "    result['time_taken'] = time.time() - start\n",
    "    \n",
    "    print \"time taken was {}\".format(result['time_taken'])\n",
    "    print \"\\n\"\n",
    "    svc_results[feature_list_to_key(features_list)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_to_df(svc_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached     2 /    17 || done    17 %  || Time taken          2 /         10\n",
      "reached     4 /    17 || done    29 %  || Time taken          3 /         10\n",
      "reached     6 /    17 || done    41 %  || Time taken          4 /         10\n",
      "reached     8 /    17 || done    53 %  || Time taken          5 /         10\n",
      "reached    10 /    17 || done    65 %  || Time taken          7 /         10\n",
      "reached    12 /    17 || done    73 %  || Time taken          8 /         10\n",
      "reached    14 /    17 || done    84 %  || Time taken          9 /         10\n",
      "reached    16 /    17 || done    95 %  || Time taken         10 /         10\n",
      "******************************\n",
      "total     17 at    10.4362\n"
     ]
    }
   ],
   "source": [
    "# def add_results_to_map(clf, dictionary, features_list, my_dataset = data_dict):\n",
    "#     current_key = feature_list_to_key(features_list)\n",
    "\n",
    "#     dictionary[current_key] = test_classifier(clf, my_dataset, features_list)\n",
    "\n",
    "def get_dict(clf, number_features = 2):\n",
    "    dictionary = {}\n",
    "    estimates = []\n",
    "    start = time.time()\n",
    "    \n",
    "    #Tried Using threads to make it faster. It was faster. \n",
    "    # Just due to some odd reason the resulting dictionary \n",
    "    # did not have anything when accessed from outside the function\n",
    "    # Maybe TODO later\n",
    "    \n",
    "    #number_threads = 3\n",
    "    #pool = Pool(number_threads)\n",
    "    #pool.map(add_results_to_map, get_combination_features(number_features))\n",
    "\n",
    "    total_iterations = get_number_combinations(number_features)\n",
    "    for i, tmp_list in enumerate(get_combination_features(number_features)):\n",
    "        \n",
    "        current_key = feature_list_to_key(tmp_list)\n",
    "        dictionary[current_key] = test_classifier(clf, my_dataset, tmp_list)\n",
    "        \n",
    "        report_time(i, total_iterations, start, estimates, 2)\n",
    "    else:\n",
    "        print(\"*\" * 30)\n",
    "        print(\"total   {:4d} at {:10.4f}\".format(i, time.time() - start))\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "def main():\n",
    "    \n",
    "    main_start = time.time()\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    #clf = SVC(kernel=\"linear\")\n",
    "\n",
    "    return get_dict(clf, number_features = 1)\n",
    "    \n",
    "#temp_results_all = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <td>0.460545</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.904091</td>\n",
       "      <td>0.378315</td>\n",
       "      <td>0.341707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         precision  recall  accuracy        f1        f2\n",
       "exercised_stock_options   0.460545   0.321  0.904091  0.378315  0.341707"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "above_threshold_at_most(dict_to_df(temp_results_all), 0.3, 0.3, 0.7, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "naive_bayes_results, naive_bayes_df = read_dictionary('features_4_raw_features_naive_bayes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <td>0.460545</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.904091</td>\n",
       "      <td>0.378315</td>\n",
       "      <td>0.341707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         precision  recall  accuracy        f1        f2\n",
       "exercised_stock_options   0.460545   0.321  0.904091  0.378315  0.341707"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "above_threshold_at_most(naive_bayes_df, 0.3, 0.3, 0.7, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dict(clf, number_features = 2, report_on = 2):\n",
    "    dictionary = {}\n",
    "    estimates = []\n",
    "    start = time.time()\n",
    "\n",
    "    total_iterations = get_number_combinations(number_features)\n",
    "    for i, tmp_list in enumerate(get_combination_features(number_features)):\n",
    "        \n",
    "        current_key = feature_list_to_key(tmp_list)\n",
    "        dictionary[current_key] = test_classifier(clf, my_dataset, tmp_list)\n",
    "        \n",
    "        report_time(i, total_iterations, start, estimates, report_on)\n",
    "    else:\n",
    "        print(\"*\" * 30)\n",
    "        print(\"total   {:4d} at {:10.4f}\".format(i, time.time() - start))\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "def main():\n",
    "    \n",
    "    main_start = time.time()\n",
    "    \n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    algorithm = KNeighborsClassifier(n_neighbors = 1)\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "#     from sklearn.decomposition import PCA\n",
    "    steps = [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('classifier', algorithm)\n",
    "    ]\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    clf = Pipeline(steps)\n",
    "\n",
    "    return get_dict(clf, number_features = 4, report_on = 5)\n",
    "    \n",
    "#temp_results_all = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>exercised_stock_options,fraction_from_this_person_to_poi,from_messages,from_poi_to_this_person</th>\n",
       "      <td>0.653677</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.623234</td>\n",
       "      <td>0.606292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exercised_stock_options,fraction_from_this_person_to_poi,from_poi_to_this_person</th>\n",
       "      <td>0.644087</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.876583</td>\n",
       "      <td>0.610366</td>\n",
       "      <td>0.591776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    precision  recall  \\\n",
       "exercised_stock_options,fraction_from_this_pers...   0.653677  0.5955   \n",
       "exercised_stock_options,fraction_from_this_pers...   0.644087  0.5800   \n",
       "\n",
       "                                                    accuracy        f1  \\\n",
       "exercised_stock_options,fraction_from_this_pers...  0.880000  0.623234   \n",
       "exercised_stock_options,fraction_from_this_pers...  0.876583  0.610366   \n",
       "\n",
       "                                                          f2  \n",
       "exercised_stock_options,fraction_from_this_pers...  0.606292  \n",
       "exercised_stock_options,fraction_from_this_pers...  0.591776  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "above_threshold_at_most(dict_to_df(temp_results_all), 0.55, 0.55, 0.85, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#store_dictionary(temp_results_all, \"min_max_kneighbors_4_features_with_3_new_features.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
