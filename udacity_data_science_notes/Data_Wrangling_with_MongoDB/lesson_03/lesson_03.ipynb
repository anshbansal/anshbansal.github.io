{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction\n",
    " - We know how to get data now we focus on data itself\n",
    " - how good is it?\n",
    " - can it be trusted?\n",
    " \n",
    "## What is Data Cleaning\n",
    " - iterative process - detect, correct and loop\n",
    " - problems in data\n",
    "     - text where we expect to find numeric data (two instead of 2)\n",
    "     - may have missing elements, extra elements or totally different structure\n",
    "     - out of range numbers\n",
    "     - outliers compared to standard distributions\n",
    "     - date entries in different formats\n",
    "     \n",
    "## Sources of dirt data\n",
    "- user entry errors\n",
    "- poor data coding standard\n",
    "- different schemas for same kind of data\n",
    "- legacy systems\n",
    "- evolving systems\n",
    "- no unique identifiers\n",
    "- lost data during data migration\n",
    "- programmer error\n",
    "- corruption in transmission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of data quality\n",
    "- Validity: Conforms to a schema or other constraints\n",
    "- Accuracy: Conforms to gold standards\n",
    "- Completeness: all records?\n",
    "- Consistency: matches other data\n",
    "- uniformity: same units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint for cleaning\n",
    "- Audit data \n",
    "    - programmatically check data and create a report on quality of data\n",
    "    - statistical analysis to check for outliers\n",
    "- create a data cleaning plan\n",
    "    - identify causes\n",
    "    - define operations to clean\n",
    "    - test to ensure operations will clean as expected\n",
    "- run the plan\n",
    "- possibly manually cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning using blueprint\n",
    " - We will use a sub set of open street map data for Chicago to see cleaning\n",
    " - We will initially take a sample of our OSM file as the main one is quite big. The main file for Chicago can be downloaded [here](https://mapzen.com/data/metro-extracts/metro/chicago_illinois/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import re\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def take_sample(k, osm_file, sample_file):\n",
    "    with open(sample_file, 'wb') as output:\n",
    "        output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "        output.write('<osm>\\n  ')\n",
    "\n",
    "        # Write every kth top level element\n",
    "        for i, element in enumerate(get_element(osm_file)):\n",
    "            if i % k == 0:\n",
    "                # print \"i is {}\".format(i)\n",
    "                output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "        output.write('</osm>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#take_sample(10, \"../chicago_illinois.osm\", \"../sample_10.osm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will audit street types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_sorted_dict(d):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print \"%s: %d\" % (k, v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(int)\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "\n",
    "        street_types[street_type] += 1\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit1(osm_file):\n",
    "    for event, elem in ET.iterparse(osm_file):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])    \n",
    "    print_sorted_dict(street_types)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30: 1\n",
      "38: 1\n",
      "59: 2\n",
      "60008: 1\n",
      "83: 1\n",
      "?: 3\n",
      "Ave: 9\n",
      "Avenue: 31504\n",
      "B: 20\n",
      "Belmont: 1\n",
      "Blvd: 4\n",
      "Boulevard: 818\n",
      "Broadway: 46\n",
      "C: 15\n",
      "Center: 2\n",
      "Cir: 11\n",
      "Circle: 18\n",
      "Court: 153\n",
      "Ct: 27\n",
      "Ct.: 1\n",
      "D: 17\n",
      "Dr: 61\n",
      "Dr.: 1\n",
      "Drive: 350\n",
      "E: 21\n",
      "East: 1\n",
      "F: 28\n",
      "G: 48\n",
      "H: 52\n",
      "Highway: 39\n",
      "J: 62\n",
      "Justamere: 1\n",
      "K: 3\n",
      "L: 86\n",
      "Lane: 65\n",
      "Ln: 8\n",
      "M: 66\n",
      "Market: 7\n",
      "N: 62\n",
      "North: 2\n",
      "O: 42\n",
      "Park: 15\n",
      "Parkway: 132\n",
      "Path: 2\n",
      "Place: 2470\n",
      "Plaza: 5\n",
      "Rd: 8\n",
      "RD: 1\n",
      "Rd.: 2\n",
      "road: 1\n",
      "Road: 662\n",
      "Row: 1\n",
      "Square: 3\n",
      "St: 22\n",
      "st: 1\n",
      "St.: 1\n",
      "Street: 13765\n",
      "Terrace: 55\n",
      "Trail: 4\n",
      "Trl: 2\n",
      "Vista: 1\n",
      "W: 1\n",
      "West: 10\n"
     ]
    }
   ],
   "source": [
    "audit1(\"../sample_10.osm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have taken the street types that the parser recognized. \n",
    "- we can see that there are multiple forms of each type. e.g. we have multiple forms of avenue.\n",
    "- now we have street types we would decide what type of cleaning do we want. Which ones do we keep and which to rename? Do we want to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing Validity\n",
    "We may have different type of validity checks\n",
    "\n",
    "- mandatory\n",
    "- unique\n",
    "- foreign key\n",
    "- cross field constraints e.g. start date must be before end date\n",
    "- fields should have specific type e.g. numerical 2 vs. two\n",
    "- patterns\n",
    "- in a range\n",
    "- should be in a set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Infobox Dataset\n",
    " - wikipedia has data infoboxes besides main article\n",
    " - [dbpedia](http://wiki.dbpedia.org/datasets) has taken data from wikipedia and shared in a way that we can download\n",
    " - we will use cities data set that contains information about all cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing a Cross-Field Constrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_row(input_file, skip_lines):\n",
    "    rows = csv.DictReader(open(input_file))\n",
    "\n",
    "    skipped = 0\n",
    "    for row in rows:\n",
    "        if skipped < skip_lines:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_number(v):\n",
    "    try:\n",
    "        float(v)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def ensure_float(v):\n",
    "    if is_number(v):\n",
    "        return float(v)\n",
    "    else:\n",
    "        print \"v is {}\".format(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def audit_population_denisty(input_file):\n",
    "    for city in get_row(input_file, 3):\n",
    "        population = ensure_float(city['populationTotal'])\n",
    "        area = ensure_float(city['areaLand'])\n",
    "        population_denisty = ensure_float(city['populationDensity'])\n",
    "        if population and area and population_denisty:\n",
    "            calculated_density = population / area\n",
    "            \n",
    "            if math.fabs(calculated_density - population_denisty) > 10:\n",
    "                print \"Possibly bad population denisty for {}\".format(city['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v is NULL\n",
      "v is NULL\n",
      "v is NULL\n",
      "v is NULL\n",
      "v is NULL\n",
      "v is NULL\n",
      "v is NULL\n",
      "v is NULL\n",
      "v is NULL\n",
      "v is NULL\n",
      "v is NULL\n",
      "v is NULL\n",
      "v is {1.01787e+08|1.019e+08}\n",
      "v is NULL\n",
      "v is {3.15979e+07|3.17e+07}\n",
      "v is {233.4|234.789}\n",
      "v is {5.51667e+07|5.53e+07}\n",
      "v is {181.159|1215.3}\n",
      "v is {6.36e+07|6.37137e+07}\n",
      "v is {176.912|177.3}\n",
      "v is {3.78138e+07|3.79e+07}\n",
      "v is {504.8|507.029}\n",
      "v is {2.02e+07|2.02019e+07}\n",
      "v is {47.1|47.1431}\n",
      "v is {2.87489e+08|2.875e+08}\n",
      "v is NULL\n",
      "v is {2.5355e+07|2.5356e+07}\n",
      "v is NULL\n",
      "v is {2.512e+08|2.51229e+08}\n",
      "v is NULL\n",
      "v is {8.25e+07|8.26206e+07}\n",
      "v is {379.114|379.7}\n",
      "v is NULL\n",
      "Possibly bad population denisty for Ketchikan Alaska\n",
      "v is {4.48e+06|4.48068e+06}\n",
      "v is {400.388|400.4}\n",
      "v is {1.458e+07|1.45816e+07}\n",
      "v is {321.2|321.237}\n",
      "v is {1.71198e+07|1.712e+07}\n",
      "v is {666.76|666.8}\n",
      "v is {2.07e+06|2.07199e+06}\n",
      "v is {1103.29|1103.3}\n",
      "v is {4.61e+06|4.61018e+06}\n",
      "v is {642.9|642.937}\n",
      "v is {3.26e+06|3.26339e+06}\n",
      "v is {985.487|985.5}\n",
      "v is {9.057e+07|9.05719e+07}\n",
      "v is {278.882|278.9}\n",
      "v is {1.274e+07|1.27427e+07}\n",
      "v is {989.657|989.7}\n",
      "v is {3.136e+07|3.13648e+07}\n",
      "v is {760.776|760.8}\n",
      "v is {5.31466e+07|5.315e+07}\n",
      "v is {965.6|965.641}\n",
      "v is {3.43173e+07|3.432e+07}\n",
      "v is {1351.98|1352.0}\n",
      "v is {5.33538e+06|5.34e+06}\n",
      "v is {318.8|318.805}\n",
      "v is {8.184e+07|8.18436e+07}\n",
      "v is {294.9|294.905}\n",
      "v is {1.13959e+07|1.14e+07}\n",
      "v is {400.0|400.04}\n",
      "v is {2.056e+07|2.05645e+07}\n",
      "v is {691.6|691.625}\n",
      "v is {9.782e+07|9.78239e+07}\n",
      "v is {917.5|917.533}\n",
      "v is NULL\n",
      "v is NULL\n",
      "v is NULL\n",
      "v is NULL\n",
      "v is NULL\n",
      "v is {4.94688e+06|4.95e+06}\n",
      "v is {150.194|150.2}\n",
      "v is {1.20175e+07|1.202e+07}\n",
      "v is {345.4|345.407}\n"
     ]
    }
   ],
   "source": [
    "audit_population_denisty('cities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting Validity\n",
    "Your task is to check the \"productionStartYear\" of the DBPedia autos datafile for valid values.\n",
    "The following things should be done:\n",
    "- check if the field \"productionStartYear\" contains a year\n",
    "- check if the year is in range 1886-2014\n",
    "- convert the value of the field to be just a year (not full datetime)\n",
    "- the rest of the fields and values should stay the same\n",
    "- if the value of the field is a valid year in the range as described above,\n",
    "  write that line to the output_good file\n",
    "- if the value of the field is not a valid year as described above,\n",
    "  write that line to the output_bad file\n",
    "- discard rows (neither write to good nor bad) if the URI is not from dbpedia.org\n",
    "- you should use the provided way of reading and writing data (DictReader and DictWriter)\n",
    "  They will take care of dealing with the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pprint\n",
    "\n",
    "def process_file(input_file, output_good, output_bad):\n",
    "\n",
    "    with open(input_file, \"r\") as f, open(output_good, 'w') as good, open(output_bad, 'w') as bad:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        header = reader.fieldnames\n",
    "\n",
    "        good_writer = csv.DictWriter(good, delimiter=\",\", fieldnames=header)\n",
    "        good_writer.writeheader()\n",
    "        bad_writer = csv.DictWriter(bad, delimiter=\",\", fieldnames=header)\n",
    "        bad_writer.writeheader()\n",
    "\n",
    "        for line in reader:\n",
    "            uri = line['URI']\n",
    "            if 'dbpedia.org' not in uri:\n",
    "                continue\n",
    "\n",
    "            production_start_year = line['productionStartYear']\n",
    "            try:\n",
    "                year = int(production_start_year[0:4])\n",
    "                good_writer.writerow(line)\n",
    "            except ValueError:\n",
    "                bad_writer.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_file('../autos.csv', 'autos-valid.csv', 'FIXME-autos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing Accuracy\n",
    " - difficult to do as needs gold standard data\n",
    "\n",
    "### Problems faced\n",
    "- some values are arrays\n",
    "- value of one column written in another column\n",
    "- underscores used - regex can be used to extract the actual data\n",
    "- possibly valid countries - in another language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing Completeness\n",
    " - can be difficult to audit as \"You don't know what you don't know\"\n",
    " - about missing records\n",
    " - need reference data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing Consistency\n",
    "- two different entries should not contradict each other\n",
    "- to fix this \n",
    "    - which data source do we trust more?\n",
    "    - which data collection method is more reliable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing Uniformity\n",
    "- All fields using same units of measurement\n",
    "- pick the type of the field and see whether it is as per your expectation or not\n",
    "- print out unexpected things to see what is happening with your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#counts is {'nulls': 0, 'empties': 0, 'arrays': 0}\n",
    "def audit_float_field(v, counts, min_val, max_val):\n",
    "    v = v.strip()\n",
    "    if v == 'NULL':\n",
    "        counts['nulls'] += 1\n",
    "    elif v == \"\":\n",
    "        counts['empties'] += 1\n",
    "    elif is_array(v):\n",
    "        counts['arrays'] += 1\n",
    "    elif not is_number(v):\n",
    "        print \"found not number {}\".format(v)\n",
    "    else:\n",
    "        v = float(v)\n",
    "        if not (min_val <= v <= max_val):\n",
    "            print \"found out of range value {}\".format(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More About Correcting Data\n",
    "- we are removing typographical errors or correcting them\n",
    "- validating against known list of entities or cross check against validated data set\n",
    "- data enhancement like N, E, W, S to North, East, West, South\n",
    "- changing reference data - 2 digit country code to 3 digit country code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM SET START HERE\n",
    "\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up. In the first exercise we want you to audit\n",
    "the datatypes that can be found in some particular fields in the dataset.\n",
    "The possible types of values can be:\n",
    "- NoneType if the value is a string \"NULL\" or an empty string \"\"\n",
    "- list, if the value starts with \"{\"\n",
    "- int, if the value can be cast to int\n",
    "- float, if the value can be cast to float, but CANNOT be cast to int.\n",
    "   For example, '3.23e+07' should be considered a float because it can be cast\n",
    "   as float but int('3.23e+07') will throw a ValueError\n",
    "- 'str', for all other values\n",
    "\n",
    "The audit_file function should return a dictionary containing fieldnames and a \n",
    "SET of the types that can be found in the field. e.g.\n",
    "{\"field1\": set([type(float()), type(int()), type(str())]),\n",
    " \"field2\": set([type(str())]),\n",
    "  ....\n",
    "}\n",
    "The type() function returns a type object describing the argument given to the \n",
    "function. You can also use examples of objects to create type objects, e.g.\n",
    "type(1.1) for a float: see the test function below for examples.\n",
    "\n",
    "Note that the first three rows (after the header row) in the cities.csv file\n",
    "are not actual data points. The contents of these rows should note be included\n",
    "when processing data types. Be sure to include functionality in your code to\n",
    "skip over or detect these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "def audit_file(file_name, fields):\n",
    "    field_types = {}\n",
    "    for field in fields:\n",
    "        field_types[field] = set()\n",
    "\n",
    "    f = open(file_name, \"r\")\n",
    "    reader = csv.DictReader(f)\n",
    "    for line in reader:\n",
    "        # print line\n",
    "        for field in fields:\n",
    "            field_value = line[field].strip()\n",
    "\n",
    "            # if field == \"name\" or field == \"areaLand\":\n",
    "            #     print \"field {} has value {}\".format(field, field_value)\n",
    "\n",
    "            if field_value == \"NULL\" or field_value == \"\":\n",
    "                field_types[field].add(type(None))\n",
    "                continue\n",
    "\n",
    "            if field_value.startswith(\"{\"):\n",
    "                field_types[field].add(type([]))\n",
    "\n",
    "            try:\n",
    "                int(field_value)\n",
    "                field_types[field].add(type(1))\n",
    "            except:\n",
    "                try:\n",
    "                    float(field_value)\n",
    "                    field_types[field].add(type(1.1))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    pprint.pprint(field_types)\n",
    "\n",
    "    return field_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FIELDS = [\"name\", \"timeZone_label\", \"utcOffset\", \"homepage\", \"governmentType_label\",\n",
    "          \"isPartOf_label\", \"areaCode\", \"populationTotal\", \"elevation\",\n",
    "          \"maximumElevation\", \"minimumElevation\", \"populationDensity\",\n",
    "          \"wgs84_pos#lat\", \"wgs84_pos#long\", \"areaLand\", \"areaMetro\", \"areaUrban\"]\n",
    "\n",
    "def test1(cities):\n",
    "    field_types = audit_file(cities, FIELDS)\n",
    "\n",
    "    assert field_types[\"areaLand\"] == {type(1.1), type([]), type(None)}\n",
    "    assert field_types['areaMetro'] == {type(1.1), type(None)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'areaCode': set([<type 'int'>, <type 'NoneType'>]),\n",
      " 'areaLand': set([<type 'float'>, <type 'list'>, <type 'NoneType'>]),\n",
      " 'areaMetro': set([<type 'float'>, <type 'NoneType'>]),\n",
      " 'areaUrban': set([<type 'float'>, <type 'NoneType'>]),\n",
      " 'elevation': set([<type 'float'>, <type 'list'>, <type 'NoneType'>]),\n",
      " 'governmentType_label': set([<type 'NoneType'>]),\n",
      " 'homepage': set([<type 'NoneType'>]),\n",
      " 'isPartOf_label': set([<type 'list'>, <type 'NoneType'>]),\n",
      " 'maximumElevation': set([<type 'NoneType'>]),\n",
      " 'minimumElevation': set([<type 'NoneType'>]),\n",
      " 'name': set([<type 'list'>, <type 'NoneType'>]),\n",
      " 'populationDensity': set([<type 'float'>, <type 'list'>, <type 'NoneType'>]),\n",
      " 'populationTotal': set([<type 'int'>, <type 'NoneType'>]),\n",
      " 'timeZone_label': set([<type 'NoneType'>]),\n",
      " 'utcOffset': set([<type 'int'>, <type 'list'>, <type 'NoneType'>]),\n",
      " 'wgs84_pos#lat': set([<type 'float'>]),\n",
      " 'wgs84_pos#long': set([<type 'float'>])}\n"
     ]
    }
   ],
   "source": [
    "test1('cities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in the previous quiz you made a decision on which value to keep for the\n",
    "\"areaLand\" field, you now know what has to be done.\n",
    "\n",
    "Finish the function fix_area(). It will receive a string as an input, and it\n",
    "has to return a float representing the value of the area or None.\n",
    "You have to change the function fix_area. You can use extra functions if you\n",
    "like, but changes to process_file will not be taken into account.\n",
    "The rest of the code is just an example on how this function can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_area(area):\n",
    "\n",
    "    result = area\n",
    "\n",
    "    if area == \"NULL\" or area == \"\":\n",
    "        result = None\n",
    "    elif area.startswith(\"{\"):\n",
    "        areas = area.strip(\"{}\").split(\"|\")\n",
    "        # area =\n",
    "        # area1 = float(areas[1])\n",
    "        result = float(areas[0])\n",
    "\n",
    "    print \"for {} output is {}\".format(area, result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous quiz you recognized that the \"name\" value can be an array (or\n",
    "list in Python terms). It would make it easier to process and query the data\n",
    "later if all values for the name are in a Python list, instead of being\n",
    "just a string separated with special characters, like now.\n",
    "\n",
    "Finish the function fix_name(). It will recieve a string as an input, and it\n",
    "will return a list of all the names. If there is only one name, the list will\n",
    "have only one item in it; if the name is \"NULL\", the list should be empty.\n",
    "The rest of the code is just an example on how this function can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_name(name):\n",
    "\n",
    "    result = name\n",
    "    if name.startswith(\"{\"):\n",
    "        result = name.strip(\"{}\").split(\"|\")\n",
    "    else:\n",
    "        result = [name]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file_fun(filename, fun):\n",
    "    data = []\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        #skipping the extra metadata\n",
    "        for i in range(3):\n",
    "            l = reader.next()\n",
    "\n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            fun(line)\n",
    "            \n",
    "            #pprint.pprint(line)\n",
    "            \n",
    "            data.append(line)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fixArea(line):\n",
    "    if \"areaLand\" in line:\n",
    "        line[\"areaLand\"] = fix_area(line[\"areaLand\"])\n",
    "\n",
    "def fixName(line):\n",
    "    if \"name\" in line:\n",
    "        line[\"name\"] = fix_name(line[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test2(cities):\n",
    "    data = process_file_fun(cities, fixArea)\n",
    "\n",
    "    print \"Printing three example results:\"\n",
    "    for n in range(5, 8):\n",
    "        pprint.pprint(data[n][\"areaLand\"])\n",
    "        \n",
    "    pprint.pprint(data[3])\n",
    "\n",
    "    assert data[3][\"areaLand\"] == None\n",
    "    assert data[8][\"areaLand\"] == 55166700.0\n",
    "    assert data[20][\"areaLand\"] == 14580000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for NULL output is None\n",
      "for NULL output is None\n",
      "for NULL output is None\n",
      "for NULL output is None\n",
      "for NULL output is None\n",
      "for NULL output is None\n",
      "for {1.01787e+08|1.019e+08} output is 101787000.0\n",
      "for {3.15979e+07|3.17e+07} output is 31597900.0\n",
      "for {5.51667e+07|5.53e+07} output is 55166700.0\n",
      "for {6.36e+07|6.37137e+07} output is 63600000.0\n",
      "for {3.78138e+07|3.79e+07} output is 37813800.0\n",
      "for {2.02e+07|2.02019e+07} output is 20200000.0\n",
      "for {2.87489e+08|2.875e+08} output is 287489000.0\n",
      "for {2.5355e+07|2.5356e+07} output is 25355000.0\n",
      "for {2.512e+08|2.51229e+08} output is 251200000.0\n",
      "for {8.25e+07|8.26206e+07} output is 82500000.0\n",
      "for 3.58195e+07 output is 3.58195e+07\n",
      "for 1.13e+07 output is 1.13e+07\n",
      "for 5.32e+07 output is 5.32e+07\n",
      "for {4.48e+06|4.48068e+06} output is 4480000.0\n",
      "for {1.458e+07|1.45816e+07} output is 14580000.0\n",
      "for {1.71198e+07|1.712e+07} output is 17119800.0\n",
      "for {2.07e+06|2.07199e+06} output is 2070000.0\n",
      "for {4.61e+06|4.61018e+06} output is 4610000.0\n",
      "for {3.26e+06|3.26339e+06} output is 3260000.0\n",
      "for {9.057e+07|9.05719e+07} output is 90570000.0\n",
      "for {1.274e+07|1.27427e+07} output is 12740000.0\n",
      "for {3.136e+07|3.13648e+07} output is 31360000.0\n",
      "for {5.31466e+07|5.315e+07} output is 53146600.0\n",
      "for {3.43173e+07|3.432e+07} output is 34317300.0\n",
      "for {5.33538e+06|5.34e+06} output is 5335380.0\n",
      "for {8.184e+07|8.18436e+07} output is 81840000.0\n",
      "for {1.13959e+07|1.14e+07} output is 11395900.0\n",
      "for {2.056e+07|2.05645e+07} output is 20560000.0\n",
      "for {9.782e+07|9.78239e+07} output is 97820000.0\n",
      "for NULL output is None\n",
      "for NULL output is None\n",
      "for {4.94688e+06|4.95e+06} output is 4946880.0\n",
      "for {1.20175e+07|1.202e+07} output is 12017500.0\n",
      "Printing three example results:\n",
      "None\n",
      "101787000.0\n",
      "31597900.0\n",
      "{'22-rdf-syntax-ns#type': '{http://dbpedia.org/ontology/City|http://dbpedia.org/ontology/Place|http://dbpedia.org/ontology/PopulatedPlace|http://dbpedia.org/ontology/Settlement|http://schema.org/City|http://schema.org/Place|http://www.opengis.net/gml/_Feature|http://www.w3.org/2002/07/owl#Thing}',\n",
      " '22-rdf-syntax-ns#type_label': '{city|place|populated place|municipality|City|Place|_Feature|owl#Thing}',\n",
      " 'URI': 'http://dbpedia.org/resource/Kumhari',\n",
      " 'administrativeDistrict': 'NULL',\n",
      " 'administrativeDistrict_label': 'NULL',\n",
      " 'anthem': 'NULL',\n",
      " 'anthem_label': 'NULL',\n",
      " 'area': 'NULL',\n",
      " 'areaCode': 'NULL',\n",
      " 'areaLand': None,\n",
      " 'areaMetro': 'NULL',\n",
      " 'areaRural': 'NULL',\n",
      " 'areaTotal': 'NULL',\n",
      " 'areaUrban': 'NULL',\n",
      " 'areaWater': 'NULL',\n",
      " 'city': 'NULL',\n",
      " 'city_label': 'NULL',\n",
      " 'code': 'NULL',\n",
      " 'country': 'http://dbpedia.org/resource/India',\n",
      " 'country_label': 'India',\n",
      " 'daylightSavingTimeZone': 'NULL',\n",
      " 'daylightSavingTimeZone_label': 'NULL',\n",
      " 'depiction': 'NULL',\n",
      " 'depiction_label': 'NULL',\n",
      " 'district': 'NULL',\n",
      " 'district_label': 'NULL',\n",
      " 'division': 'NULL',\n",
      " 'division_label': 'NULL',\n",
      " 'elevation': '285.0',\n",
      " 'federalState': 'NULL',\n",
      " 'federalState_label': 'NULL',\n",
      " 'foundingDate': 'NULL',\n",
      " 'foundingPerson': 'NULL',\n",
      " 'foundingPerson_label': 'NULL',\n",
      " 'foundingYear': 'NULL',\n",
      " 'governingBody': 'NULL',\n",
      " 'governingBody_label': 'NULL',\n",
      " 'government': 'NULL',\n",
      " 'governmentType': 'NULL',\n",
      " 'governmentType_label': 'NULL',\n",
      " 'government_label': 'NULL',\n",
      " 'homepage': 'NULL',\n",
      " 'homepage_label': 'NULL',\n",
      " 'isPartOf': '{http://dbpedia.org/resource/Chhattisgarh|http://dbpedia.org/resource/Durg_district}',\n",
      " 'isPartOf_label': '{Chhattisgarh|Durg district}',\n",
      " 'isoCodeRegion': 'NULL',\n",
      " 'isoCodeRegion_label': 'NULL',\n",
      " 'leader': 'NULL',\n",
      " 'leaderName': 'NULL',\n",
      " 'leaderName_label': 'NULL',\n",
      " 'leaderParty': 'NULL',\n",
      " 'leaderParty_label': 'NULL',\n",
      " 'leaderTitle': 'NULL',\n",
      " 'leader_label': 'NULL',\n",
      " 'location': 'NULL',\n",
      " 'location_label': 'NULL',\n",
      " 'maximumElevation': 'NULL',\n",
      " 'mayor': 'NULL',\n",
      " 'mayor_label': 'NULL',\n",
      " 'minimumElevation': 'NULL',\n",
      " 'motto': 'NULL',\n",
      " 'municipality': 'NULL',\n",
      " 'municipality_label': 'NULL',\n",
      " 'name': 'Kumhari',\n",
      " 'nick': 'NULL',\n",
      " 'part': 'NULL',\n",
      " 'part_label': 'NULL',\n",
      " 'percentageOfAreaWater': 'NULL',\n",
      " 'point': '21.27 81.52',\n",
      " 'populationAsOf': 'NULL',\n",
      " 'populationDensity': 'NULL',\n",
      " 'populationMetro': 'NULL',\n",
      " 'populationMetroDensity': 'NULL',\n",
      " 'populationRural': 'NULL',\n",
      " 'populationTotal': '29737',\n",
      " 'populationTotalRanking': 'NULL',\n",
      " 'populationUrban': 'NULL',\n",
      " 'populationUrbanDensity': 'NULL',\n",
      " 'postalCode': 'NULL',\n",
      " 'rdf-schema#comment': 'Kumhari is a town and a nagar panchayat in Durg district in the Indian state of Chhattisgarh.',\n",
      " 'rdf-schema#label': 'Kumhari',\n",
      " 'region': 'NULL',\n",
      " 'region_label': 'NULL',\n",
      " 'state': 'NULL',\n",
      " 'state_label': 'NULL',\n",
      " 'synonym': 'NULL',\n",
      " 'thumbnail': 'NULL',\n",
      " 'thumbnail_label': 'NULL',\n",
      " 'timeZone': 'http://dbpedia.org/resource/Indian_Standard_Time',\n",
      " 'timeZone_label': 'Indian Standard Time',\n",
      " 'twinCity': 'NULL',\n",
      " 'twinCity_label': 'NULL',\n",
      " 'twinCountry': 'NULL',\n",
      " 'twinCountry_label': 'NULL',\n",
      " 'type': 'NULL',\n",
      " 'type_label': 'NULL',\n",
      " 'utcOffset': '+5:30',\n",
      " 'wgs84_pos#lat': '21.27',\n",
      " 'wgs84_pos#long': '81.52'}\n"
     ]
    }
   ],
   "source": [
    "test2('cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test3(cities):\n",
    "    data = process_file_fun(cities, fixName)\n",
    "\n",
    "    #print \"Printing 20 results:\"\n",
    "    #for n in range(20):\n",
    "    #    pprint.pprint(data[n][\"name\"])\n",
    "    #pprint.pprint(\"data[14] is {}\".format(data[14]))\n",
    "\n",
    "    assert data[14][\"name\"] == ['Negtemiut', 'Nightmute']\n",
    "    assert data[9][\"name\"] == ['Pell City Alabama']\n",
    "    assert data[3][\"name\"] == ['Kumhari']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test3(\"cities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the full city data, you will notice that there are couple of\n",
    "values that seem to provide the same information in different formats: \"point\"\n",
    "seems to be the combination of \"wgs84_pos#lat\" and \"wgs84_pos#long\". However,\n",
    "we do not know if that is the case and should check if they are equivalent.\n",
    "\n",
    "Finish the function check_loc(). It will recieve 3 strings: first, the combined\n",
    "value of \"point\" followed by the separate \"wgs84_pos#\" values. You have to\n",
    "extract the lat and long values from the \"point\" argument and compare them to\n",
    "the \"wgs84_pos# values, returning True or False.\n",
    "\n",
    "Note that you do not have to fix the values, only determine if they are\n",
    "consistent. To fix them in this case you would need more information. Feel free\n",
    "to discuss possible strategies for fixing this on the discussion forum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
